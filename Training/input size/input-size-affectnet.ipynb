{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7777077,"sourceType":"datasetVersion","datasetId":4272440}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nDATA_PATH = \"/kaggle/input/fyp-dataset/AffectNet/\"\nTOP_EMOTIONS = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n\nTRAIN_SIZE = 0.80\nNUM_CLASSES = len(TOP_EMOTIONS)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T16:39:51.164452Z","iopub.execute_input":"2024-03-19T16:39:51.164810Z","iopub.status.idle":"2024-03-19T16:40:06.610708Z","shell.execute_reply.started":"2024-03-19T16:39:51.164783Z","shell.execute_reply":"2024-03-19T16:40:06.609572Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-19 16:39:55.108816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-19 16:39:55.108931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-19 16:39:55.265581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (48, 48, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (48, 48, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:40:06.612889Z","iopub.execute_input":"2024-03-19T16:40:06.613644Z","iopub.status.idle":"2024-03-19T16:46:54.204823Z","shell.execute_reply.started":"2024-03-19T16:40:06.613604Z","shell.execute_reply":"2024-03-19T16:46:54.203620Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"surprise has 1851 number of images\nfear has 1839 number of images\nneutral has 1880 number of images\nsad has 1821 number of images\ndisgust has 1740 number of images\ncontempt has 1833 number of images\nhappy has 1862 number of images\nanger has 1822 number of images\n\ntotal images are 14648\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded contempt images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded anger images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:46:54.206334Z","iopub.execute_input":"2024-03-19T16:46:54.207315Z","iopub.status.idle":"2024-03-19T16:46:54.701016Z","shell.execute_reply.started":"2024-03-19T16:46:54.207277Z","shell.execute_reply":"2024-03-19T16:46:54.699990Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:46:54.703904Z","iopub.execute_input":"2024-03-19T16:46:54.704354Z","iopub.status.idle":"2024-03-19T16:50:22.881208Z","shell.execute_reply.started":"2024-03-19T16:46:54.704317Z","shell.execute_reply":"2024-03-19T16:50:22.880053Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training model with input shape: (48, 48, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 71ms/step - accuracy: 0.2281 - loss: 1.9283 - val_accuracy: 0.3836 - val_loss: 1.5705\nEpoch 2/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.4094 - loss: 1.5224 - val_accuracy: 0.4137 - val_loss: 1.4640\nEpoch 3/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.4524 - loss: 1.4001 - val_accuracy: 0.4502 - val_loss: 1.4225\nEpoch 4/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 49ms/step - accuracy: 0.4900 - loss: 1.3301 - val_accuracy: 0.4679 - val_loss: 1.3762\nEpoch 5/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.5267 - loss: 1.2462 - val_accuracy: 0.4761 - val_loss: 1.3696\nEpoch 6/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5466 - loss: 1.1921 - val_accuracy: 0.4911 - val_loss: 1.3529\nEpoch 7/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.5743 - loss: 1.1419 - val_accuracy: 0.4983 - val_loss: 1.3374\nEpoch 8/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.5973 - loss: 1.0780 - val_accuracy: 0.4980 - val_loss: 1.3467\nEpoch 9/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.6232 - loss: 1.0096 - val_accuracy: 0.5116 - val_loss: 1.3774\nEpoch 10/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.6529 - loss: 0.9450 - val_accuracy: 0.5205 - val_loss: 1.3820\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5263 - loss: 1.3975\nTest loss: 1.382006049156189, Test accuracy: 0.520477831363678\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (64, 64, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (64, 64, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:50:22.882917Z","iopub.execute_input":"2024-03-19T16:50:22.884275Z","iopub.status.idle":"2024-03-19T16:54:58.812072Z","shell.execute_reply.started":"2024-03-19T16:50:22.884225Z","shell.execute_reply":"2024-03-19T16:54:58.810899Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"surprise has 1851 number of images\nfear has 1839 number of images\nneutral has 1880 number of images\nsad has 1821 number of images\ndisgust has 1740 number of images\ncontempt has 1833 number of images\nhappy has 1862 number of images\nanger has 1822 number of images\n\ntotal images are 14648\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded contempt images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded anger images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:54:58.814065Z","iopub.execute_input":"2024-03-19T16:54:58.814830Z","iopub.status.idle":"2024-03-19T16:54:59.581774Z","shell.execute_reply.started":"2024-03-19T16:54:58.814792Z","shell.execute_reply":"2024-03-19T16:54:59.580584Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:54:59.584340Z","iopub.execute_input":"2024-03-19T16:54:59.585039Z","iopub.status.idle":"2024-03-19T17:00:59.315760Z","shell.execute_reply.started":"2024-03-19T16:54:59.585001Z","shell.execute_reply":"2024-03-19T17:00:59.314273Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training model with input shape: (64, 64, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 86ms/step - accuracy: 0.2499 - loss: 1.8828 - val_accuracy: 0.3877 - val_loss: 1.5159\nEpoch 2/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 86ms/step - accuracy: 0.4370 - loss: 1.4500 - val_accuracy: 0.4608 - val_loss: 1.3833\nEpoch 3/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 88ms/step - accuracy: 0.4881 - loss: 1.3166 - val_accuracy: 0.4915 - val_loss: 1.3373\nEpoch 4/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 91ms/step - accuracy: 0.5376 - loss: 1.2257 - val_accuracy: 0.4816 - val_loss: 1.3632\nEpoch 5/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 90ms/step - accuracy: 0.5856 - loss: 1.0985 - val_accuracy: 0.5140 - val_loss: 1.2868\nEpoch 6/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.6432 - loss: 0.9844 - val_accuracy: 0.5140 - val_loss: 1.3156\nEpoch 7/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.6954 - loss: 0.8511 - val_accuracy: 0.5229 - val_loss: 1.3172\nEpoch 8/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 84ms/step - accuracy: 0.7446 - loss: 0.7232 - val_accuracy: 0.5328 - val_loss: 1.3874\nEpoch 9/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 86ms/step - accuracy: 0.7918 - loss: 0.5980 - val_accuracy: 0.5287 - val_loss: 1.4640\nEpoch 10/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 86ms/step - accuracy: 0.8367 - loss: 0.4877 - val_accuracy: 0.5413 - val_loss: 1.5507\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5295 - loss: 1.5769\nTest loss: 1.5507205724716187, Test accuracy: 0.5412968993186951\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (128, 128, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (128, 128, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:00:59.317513Z","iopub.execute_input":"2024-03-19T17:00:59.317964Z","iopub.status.idle":"2024-03-19T17:05:55.741983Z","shell.execute_reply.started":"2024-03-19T17:00:59.317922Z","shell.execute_reply":"2024-03-19T17:05:55.740411Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"surprise has 1851 number of images\nfear has 1839 number of images\nneutral has 1880 number of images\nsad has 1821 number of images\ndisgust has 1740 number of images\ncontempt has 1833 number of images\nhappy has 1862 number of images\nanger has 1822 number of images\n\ntotal images are 14648\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded contempt images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded anger images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:05:55.746801Z","iopub.execute_input":"2024-03-19T17:05:55.747190Z","iopub.status.idle":"2024-03-19T17:05:57.772060Z","shell.execute_reply.started":"2024-03-19T17:05:55.747145Z","shell.execute_reply":"2024-03-19T17:05:57.770963Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:05:57.774109Z","iopub.execute_input":"2024-03-19T17:05:57.774600Z","iopub.status.idle":"2024-03-19T17:32:47.363477Z","shell.execute_reply.started":"2024-03-19T17:05:57.774563Z","shell.execute_reply":"2024-03-19T17:32:47.362137Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training model with input shape: (128, 128, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 419ms/step - accuracy: 0.2503 - loss: 1.9219 - val_accuracy: 0.4307 - val_loss: 1.4608\nEpoch 2/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 388ms/step - accuracy: 0.4555 - loss: 1.4173 - val_accuracy: 0.4894 - val_loss: 1.3466\nEpoch 3/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 379ms/step - accuracy: 0.5463 - loss: 1.1906 - val_accuracy: 0.5075 - val_loss: 1.3165\nEpoch 4/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 385ms/step - accuracy: 0.6253 - loss: 1.0342 - val_accuracy: 0.5276 - val_loss: 1.3269\nEpoch 5/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 425ms/step - accuracy: 0.7085 - loss: 0.8138 - val_accuracy: 0.5491 - val_loss: 1.3484\nEpoch 6/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 378ms/step - accuracy: 0.7966 - loss: 0.5939 - val_accuracy: 0.5546 - val_loss: 1.4265\nEpoch 7/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 376ms/step - accuracy: 0.8693 - loss: 0.3977 - val_accuracy: 0.5686 - val_loss: 1.6014\nEpoch 8/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 379ms/step - accuracy: 0.9190 - loss: 0.2633 - val_accuracy: 0.5737 - val_loss: 1.8486\nEpoch 9/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 380ms/step - accuracy: 0.9566 - loss: 0.1546 - val_accuracy: 0.5860 - val_loss: 2.1263\nEpoch 10/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 388ms/step - accuracy: 0.9723 - loss: 0.1096 - val_accuracy: 0.5809 - val_loss: 2.3321\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.5683 - loss: 2.4534\nTest loss: 2.332122564315796, Test accuracy: 0.5808873772621155\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (256, 256, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (256, 256, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:32:47.365901Z","iopub.execute_input":"2024-03-19T17:32:47.366377Z","iopub.status.idle":"2024-03-19T17:38:07.954492Z","shell.execute_reply.started":"2024-03-19T17:32:47.366327Z","shell.execute_reply":"2024-03-19T17:38:07.953367Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"surprise has 1851 number of images\nfear has 1839 number of images\nneutral has 1880 number of images\nsad has 1821 number of images\ndisgust has 1740 number of images\ncontempt has 1833 number of images\nhappy has 1862 number of images\nanger has 1822 number of images\n\ntotal images are 14648\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded contempt images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded anger images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:38:07.958789Z","iopub.execute_input":"2024-03-19T17:38:07.959107Z","iopub.status.idle":"2024-03-19T17:38:20.352517Z","shell.execute_reply.started":"2024-03-19T17:38:07.959082Z","shell.execute_reply":"2024-03-19T17:38:20.351568Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:38:20.353997Z","iopub.execute_input":"2024-03-19T17:38:20.354886Z","iopub.status.idle":"2024-03-19T19:32:50.750962Z","shell.execute_reply.started":"2024-03-19T17:38:20.354845Z","shell.execute_reply":"2024-03-19T19:32:50.749415Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training model with input shape: (256, 256, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 2s/step - accuracy: 0.2478 - loss: 2.1886 - val_accuracy: 0.4283 - val_loss: 1.4926\nEpoch 2/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 2s/step - accuracy: 0.4956 - loss: 1.3495 - val_accuracy: 0.5027 - val_loss: 1.3582\nEpoch 3/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2s/step - accuracy: 0.6484 - loss: 1.0028 - val_accuracy: 0.5157 - val_loss: 1.3612\nEpoch 4/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2s/step - accuracy: 0.7911 - loss: 0.6145 - val_accuracy: 0.5386 - val_loss: 1.5398\nEpoch 5/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 2s/step - accuracy: 0.9087 - loss: 0.3026 - val_accuracy: 0.5597 - val_loss: 1.9340\nEpoch 6/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 2s/step - accuracy: 0.9646 - loss: 0.1383 - val_accuracy: 0.5573 - val_loss: 2.4590\nEpoch 7/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 2s/step - accuracy: 0.9837 - loss: 0.0750 - val_accuracy: 0.5567 - val_loss: 2.5719\nEpoch 8/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 2s/step - accuracy: 0.9913 - loss: 0.0511 - val_accuracy: 0.5464 - val_loss: 2.8191\nEpoch 9/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 2s/step - accuracy: 0.9936 - loss: 0.0442 - val_accuracy: 0.5567 - val_loss: 3.1184\nEpoch 10/10\n\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 2s/step - accuracy: 0.9937 - loss: 0.0474 - val_accuracy: 0.5652 - val_loss: 2.9978\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 542ms/step - accuracy: 0.5612 - loss: 3.0558\nTest loss: 2.997849464416504, Test accuracy: 0.5651876926422119\n","output_type":"stream"}]}]}