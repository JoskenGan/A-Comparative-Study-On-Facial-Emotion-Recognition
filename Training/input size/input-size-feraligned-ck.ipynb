{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7777077,"sourceType":"datasetVersion","datasetId":4272440}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nDATA_PATH = \"/kaggle/input/fyp-dataset/feralignedck/feraligned+ck/\"\nTOP_EMOTIONS = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sadness\", \"Surprise\"]\n\nTRAIN_SIZE = 0.80\nNUM_CLASSES = len(TOP_EMOTIONS)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T14:55:13.351750Z","iopub.execute_input":"2024-03-19T14:55:13.352171Z","iopub.status.idle":"2024-03-19T14:55:28.397017Z","shell.execute_reply.started":"2024-03-19T14:55:13.352139Z","shell.execute_reply":"2024-03-19T14:55:28.395854Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-19 14:55:16.396636: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-19 14:55:16.396765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-19 14:55:16.546326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (48, 48, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (48, 48, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:55:45.758887Z","iopub.execute_input":"2024-03-19T14:55:45.759675Z","iopub.status.idle":"2024-03-19T14:56:32.957363Z","shell.execute_reply.started":"2024-03-19T14:55:45.759612Z","shell.execute_reply":"2024-03-19T14:56:32.956245Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Surprise has 249 number of images\nFear has 728 number of images\nAngry has 938 number of images\nNeutral has 1230 number of images\nSadness has 1153 number of images\nHappy has 2203 number of images\n\ntotal images are 6501\nloaded Surprise images to numpy arrays...\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:56:32.958974Z","iopub.execute_input":"2024-03-19T14:56:32.959333Z","iopub.status.idle":"2024-03-19T14:56:33.170564Z","shell.execute_reply.started":"2024-03-19T14:56:32.959307Z","shell.execute_reply":"2024-03-19T14:56:33.169336Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:56:33.171812Z","iopub.execute_input":"2024-03-19T14:56:33.172210Z","iopub.status.idle":"2024-03-19T14:58:03.854891Z","shell.execute_reply.started":"2024-03-19T14:56:33.172179Z","shell.execute_reply":"2024-03-19T14:58:03.853930Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training model with input shape: (48, 48, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 0.3854 - loss: 1.5376 - val_accuracy: 0.5573 - val_loss: 1.1319\nEpoch 2/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.5911 - loss: 1.0494 - val_accuracy: 0.6395 - val_loss: 1.0200\nEpoch 3/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.6511 - loss: 0.9326 - val_accuracy: 0.6364 - val_loss: 0.9377\nEpoch 4/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.6947 - loss: 0.8161 - val_accuracy: 0.6672 - val_loss: 0.8613\nEpoch 5/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 0.7126 - loss: 0.7544 - val_accuracy: 0.6987 - val_loss: 0.8219\nEpoch 6/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.7602 - loss: 0.6473 - val_accuracy: 0.7133 - val_loss: 0.8207\nEpoch 7/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.7876 - loss: 0.5972 - val_accuracy: 0.6933 - val_loss: 0.8251\nEpoch 8/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.8102 - loss: 0.5340 - val_accuracy: 0.7171 - val_loss: 0.8281\nEpoch 9/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.8422 - loss: 0.4413 - val_accuracy: 0.7002 - val_loss: 0.8673\nEpoch 10/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.8703 - loss: 0.3853 - val_accuracy: 0.6987 - val_loss: 0.9238\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7005 - loss: 0.9121\nTest loss: 0.9237546920776367, Test accuracy: 0.6986933350563049\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (64, 64, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (64, 64, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:58:03.859000Z","iopub.execute_input":"2024-03-19T14:58:03.859389Z","iopub.status.idle":"2024-03-19T14:58:11.503100Z","shell.execute_reply.started":"2024-03-19T14:58:03.859358Z","shell.execute_reply":"2024-03-19T14:58:11.501855Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Surprise has 249 number of images\nFear has 728 number of images\nAngry has 938 number of images\nNeutral has 1230 number of images\nSadness has 1153 number of images\nHappy has 2203 number of images\n\ntotal images are 6501\nloaded Surprise images to numpy arrays...\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:58:11.506979Z","iopub.execute_input":"2024-03-19T14:58:11.507333Z","iopub.status.idle":"2024-03-19T14:58:11.771868Z","shell.execute_reply.started":"2024-03-19T14:58:11.507305Z","shell.execute_reply":"2024-03-19T14:58:11.770826Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:58:11.773402Z","iopub.execute_input":"2024-03-19T14:58:11.773746Z","iopub.status.idle":"2024-03-19T15:01:20.151709Z","shell.execute_reply.started":"2024-03-19T14:58:11.773718Z","shell.execute_reply":"2024-03-19T15:01:20.150529Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training model with input shape: (64, 64, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.3672 - loss: 1.5544 - val_accuracy: 0.5542 - val_loss: 1.1148\nEpoch 2/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.5892 - loss: 1.0365 - val_accuracy: 0.6418 - val_loss: 0.9461\nEpoch 3/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.6465 - loss: 0.9060 - val_accuracy: 0.6802 - val_loss: 0.8814\nEpoch 4/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 89ms/step - accuracy: 0.6984 - loss: 0.7925 - val_accuracy: 0.7071 - val_loss: 0.8290\nEpoch 5/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - accuracy: 0.7402 - loss: 0.6982 - val_accuracy: 0.6964 - val_loss: 0.8563\nEpoch 6/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 86ms/step - accuracy: 0.7808 - loss: 0.6146 - val_accuracy: 0.6902 - val_loss: 0.8666\nEpoch 7/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.8003 - loss: 0.5459 - val_accuracy: 0.6964 - val_loss: 0.8495\nEpoch 8/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.8268 - loss: 0.4812 - val_accuracy: 0.6995 - val_loss: 0.8666\nEpoch 9/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.8567 - loss: 0.4197 - val_accuracy: 0.7041 - val_loss: 0.9073\nEpoch 10/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.8768 - loss: 0.3611 - val_accuracy: 0.7048 - val_loss: 0.9513\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7243 - loss: 0.9049\nTest loss: 0.9512670636177063, Test accuracy: 0.7048424482345581\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (128, 128, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (128, 128, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:01:20.153114Z","iopub.execute_input":"2024-03-19T15:01:20.153652Z","iopub.status.idle":"2024-03-19T15:01:27.360093Z","shell.execute_reply.started":"2024-03-19T15:01:20.153598Z","shell.execute_reply":"2024-03-19T15:01:27.358706Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Surprise has 249 number of images\nFear has 728 number of images\nAngry has 938 number of images\nNeutral has 1230 number of images\nSadness has 1153 number of images\nHappy has 2203 number of images\n\ntotal images are 6501\nloaded Surprise images to numpy arrays...\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:01:27.361336Z","iopub.execute_input":"2024-03-19T15:01:27.361704Z","iopub.status.idle":"2024-03-19T15:01:28.205628Z","shell.execute_reply.started":"2024-03-19T15:01:27.361675Z","shell.execute_reply":"2024-03-19T15:01:28.204632Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:01:28.207122Z","iopub.execute_input":"2024-03-19T15:01:28.207431Z","iopub.status.idle":"2024-03-19T15:14:39.588259Z","shell.execute_reply.started":"2024-03-19T15:01:28.207404Z","shell.execute_reply":"2024-03-19T15:14:39.587381Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training model with input shape: (128, 128, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 395ms/step - accuracy: 0.3507 - loss: 1.6058 - val_accuracy: 0.5534 - val_loss: 1.1325\nEpoch 2/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 393ms/step - accuracy: 0.5883 - loss: 1.0678 - val_accuracy: 0.6103 - val_loss: 1.0510\nEpoch 3/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 396ms/step - accuracy: 0.6526 - loss: 0.9057 - val_accuracy: 0.6503 - val_loss: 0.9334\nEpoch 4/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 397ms/step - accuracy: 0.7154 - loss: 0.7631 - val_accuracy: 0.6726 - val_loss: 0.8916\nEpoch 5/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 396ms/step - accuracy: 0.7659 - loss: 0.6366 - val_accuracy: 0.6810 - val_loss: 0.8965\nEpoch 6/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 396ms/step - accuracy: 0.8053 - loss: 0.5314 - val_accuracy: 0.6672 - val_loss: 0.9949\nEpoch 7/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 393ms/step - accuracy: 0.8463 - loss: 0.4392 - val_accuracy: 0.6710 - val_loss: 1.0450\nEpoch 8/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 397ms/step - accuracy: 0.8860 - loss: 0.3290 - val_accuracy: 0.6656 - val_loss: 1.1191\nEpoch 9/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 397ms/step - accuracy: 0.9237 - loss: 0.2271 - val_accuracy: 0.6772 - val_loss: 1.2586\nEpoch 10/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 396ms/step - accuracy: 0.9515 - loss: 0.1484 - val_accuracy: 0.6749 - val_loss: 1.4766\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - accuracy: 0.6875 - loss: 1.4803\nTest loss: 1.4766212701797485, Test accuracy: 0.6748654842376709\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (256, 256, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (256, 256, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:15:31.164624Z","iopub.execute_input":"2024-03-19T15:15:31.165387Z","iopub.status.idle":"2024-03-19T15:15:45.547874Z","shell.execute_reply.started":"2024-03-19T15:15:31.165348Z","shell.execute_reply":"2024-03-19T15:15:45.546619Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Surprise has 249 number of images\nFear has 728 number of images\nAngry has 938 number of images\nNeutral has 1230 number of images\nSadness has 1153 number of images\nHappy has 2203 number of images\n\ntotal images are 6501\nloaded Surprise images to numpy arrays...\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:15:45.550488Z","iopub.execute_input":"2024-03-19T15:15:45.550971Z","iopub.status.idle":"2024-03-19T15:15:48.691303Z","shell.execute_reply.started":"2024-03-19T15:15:45.550933Z","shell.execute_reply":"2024-03-19T15:15:48.689969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:15:48.692737Z","iopub.execute_input":"2024-03-19T15:15:48.693084Z","iopub.status.idle":"2024-03-19T16:09:40.012357Z","shell.execute_reply.started":"2024-03-19T15:15:48.693054Z","shell.execute_reply":"2024-03-19T16:09:40.010922Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training model with input shape: (256, 256, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 2s/step - accuracy: 0.3162 - loss: 2.4323 - val_accuracy: 0.3951 - val_loss: 1.3891\nEpoch 2/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 2s/step - accuracy: 0.4826 - loss: 1.3034 - val_accuracy: 0.5473 - val_loss: 1.1819\nEpoch 3/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 2s/step - accuracy: 0.5716 - loss: 1.0854 - val_accuracy: 0.5450 - val_loss: 1.1812\nEpoch 4/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 2s/step - accuracy: 0.6285 - loss: 0.9695 - val_accuracy: 0.6018 - val_loss: 1.0281\nEpoch 5/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 2s/step - accuracy: 0.6961 - loss: 0.8091 - val_accuracy: 0.6218 - val_loss: 1.0508\nEpoch 6/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 2s/step - accuracy: 0.7183 - loss: 0.7464 - val_accuracy: 0.6311 - val_loss: 0.9958\nEpoch 7/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 2s/step - accuracy: 0.7705 - loss: 0.6351 - val_accuracy: 0.6541 - val_loss: 0.9617\nEpoch 8/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 2s/step - accuracy: 0.8141 - loss: 0.5213 - val_accuracy: 0.6503 - val_loss: 1.0297\nEpoch 9/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 2s/step - accuracy: 0.8559 - loss: 0.4295 - val_accuracy: 0.6526 - val_loss: 1.0681\nEpoch 10/10\n\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - accuracy: 0.8775 - loss: 0.3530 - val_accuracy: 0.6426 - val_loss: 1.2126\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 484ms/step - accuracy: 0.6608 - loss: 1.1726\nTest loss: 1.2125740051269531, Test accuracy: 0.6425826549530029\n","output_type":"stream"}]}]}