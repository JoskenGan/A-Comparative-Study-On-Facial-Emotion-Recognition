{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7777077,"sourceType":"datasetVersion","datasetId":4272440}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nDATA_PATH = \"/kaggle/input/fyp-dataset/feraligned/feraligned/\"\nTOP_EMOTIONS = [\"Angry\", \"Fear\", \"Happy\", \"Neutral\", \"Sadness\"]\n\nTRAIN_SIZE = 0.80\nNUM_CLASSES = len(TOP_EMOTIONS)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:16:05.303037Z","iopub.execute_input":"2024-03-19T13:16:05.303503Z","iopub.status.idle":"2024-03-19T13:16:05.310871Z","shell.execute_reply.started":"2024-03-19T13:16:05.303472Z","shell.execute_reply":"2024-03-19T13:16:05.309790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (48, 48, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (48, 48, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:16:05.314233Z","iopub.execute_input":"2024-03-19T13:16:05.315171Z","iopub.status.idle":"2024-03-19T13:16:11.361030Z","shell.execute_reply.started":"2024-03-19T13:16:05.315120Z","shell.execute_reply":"2024-03-19T13:16:11.360167Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fear has 552 number of images\nAngry has 633 number of images\nNeutral has 863 number of images\nSadness has 904 number of images\nHappy has 1577 number of images\n\ntotal images are 4529\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:16:11.362867Z","iopub.execute_input":"2024-03-19T13:16:11.363238Z","iopub.status.idle":"2024-03-19T13:16:11.630659Z","shell.execute_reply.started":"2024-03-19T13:16:11.363207Z","shell.execute_reply":"2024-03-19T13:16:11.629677Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:16:11.633169Z","iopub.execute_input":"2024-03-19T13:16:11.633617Z","iopub.status.idle":"2024-03-19T13:17:31.471554Z","shell.execute_reply.started":"2024-03-19T13:16:11.633568Z","shell.execute_reply":"2024-03-19T13:17:31.470124Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training model with input shape: (48, 48, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - accuracy: 0.3600 - loss: 1.5051 - val_accuracy: 0.4912 - val_loss: 1.2462\nEpoch 2/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.5653 - loss: 1.1302 - val_accuracy: 0.5949 - val_loss: 1.0651\nEpoch 3/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.6135 - loss: 0.9906 - val_accuracy: 0.5960 - val_loss: 1.0411\nEpoch 4/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.6594 - loss: 0.8935 - val_accuracy: 0.6490 - val_loss: 0.9726\nEpoch 5/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.7033 - loss: 0.7977 - val_accuracy: 0.6534 - val_loss: 0.9331\nEpoch 6/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - accuracy: 0.7288 - loss: 0.7183 - val_accuracy: 0.6634 - val_loss: 0.9305\nEpoch 7/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.7555 - loss: 0.6765 - val_accuracy: 0.6490 - val_loss: 0.9420\nEpoch 8/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.7727 - loss: 0.5954 - val_accuracy: 0.6578 - val_loss: 0.9696\nEpoch 9/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.8153 - loss: 0.5211 - val_accuracy: 0.6634 - val_loss: 0.9527\nEpoch 10/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.8341 - loss: 0.4728 - val_accuracy: 0.6689 - val_loss: 0.9445\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6709 - loss: 0.9386\nTest loss: 0.94448322057724, Test accuracy: 0.6688741445541382\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (64, 64, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (64, 64, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:17:31.473288Z","iopub.execute_input":"2024-03-19T13:17:31.473666Z","iopub.status.idle":"2024-03-19T13:17:38.777279Z","shell.execute_reply.started":"2024-03-19T13:17:31.473634Z","shell.execute_reply":"2024-03-19T13:17:38.776403Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fear has 552 number of images\nAngry has 633 number of images\nNeutral has 863 number of images\nSadness has 904 number of images\nHappy has 1577 number of images\n\ntotal images are 4529\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:17:38.778750Z","iopub.execute_input":"2024-03-19T13:17:38.779325Z","iopub.status.idle":"2024-03-19T13:17:39.082175Z","shell.execute_reply.started":"2024-03-19T13:17:38.779294Z","shell.execute_reply":"2024-03-19T13:17:39.081087Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:17:39.083836Z","iopub.execute_input":"2024-03-19T13:17:39.084249Z","iopub.status.idle":"2024-03-19T13:20:27.513177Z","shell.execute_reply.started":"2024-03-19T13:17:39.084215Z","shell.execute_reply":"2024-03-19T13:20:27.511664Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training model with input shape: (64, 64, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3519 - loss: 1.5220 - val_accuracy: 0.4581 - val_loss: 1.4331\nEpoch 2/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 96ms/step - accuracy: 0.5254 - loss: 1.2023 - val_accuracy: 0.5651 - val_loss: 1.1099\nEpoch 3/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 93ms/step - accuracy: 0.6252 - loss: 1.0050 - val_accuracy: 0.6004 - val_loss: 1.0445\nEpoch 4/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - accuracy: 0.6530 - loss: 0.9147 - val_accuracy: 0.6236 - val_loss: 1.0300\nEpoch 5/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - accuracy: 0.6759 - loss: 0.8325 - val_accuracy: 0.6391 - val_loss: 0.9593\nEpoch 6/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.7252 - loss: 0.7415 - val_accuracy: 0.6369 - val_loss: 0.9601\nEpoch 7/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - accuracy: 0.7644 - loss: 0.6615 - val_accuracy: 0.6556 - val_loss: 0.9651\nEpoch 8/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - accuracy: 0.7748 - loss: 0.6178 - val_accuracy: 0.6435 - val_loss: 0.9968\nEpoch 9/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 93ms/step - accuracy: 0.8227 - loss: 0.5045 - val_accuracy: 0.6534 - val_loss: 0.9698\nEpoch 10/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - accuracy: 0.8404 - loss: 0.4492 - val_accuracy: 0.6611 - val_loss: 1.0288\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6529 - loss: 1.0368\nTest loss: 1.0288468599319458, Test accuracy: 0.6611478924751282\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (128, 128, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (128, 128, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:20:27.515322Z","iopub.execute_input":"2024-03-19T13:20:27.515875Z","iopub.status.idle":"2024-03-19T13:20:33.408941Z","shell.execute_reply.started":"2024-03-19T13:20:27.515826Z","shell.execute_reply":"2024-03-19T13:20:33.407621Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Fear has 552 number of images\nAngry has 633 number of images\nNeutral has 863 number of images\nSadness has 904 number of images\nHappy has 1577 number of images\n\ntotal images are 4529\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:20:33.410621Z","iopub.execute_input":"2024-03-19T13:20:33.411105Z","iopub.status.idle":"2024-03-19T13:20:34.838726Z","shell.execute_reply.started":"2024-03-19T13:20:33.411058Z","shell.execute_reply":"2024-03-19T13:20:34.837552Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:20:34.841754Z","iopub.execute_input":"2024-03-19T13:20:34.842105Z","iopub.status.idle":"2024-03-19T13:32:08.400746Z","shell.execute_reply.started":"2024-03-19T13:20:34.842075Z","shell.execute_reply":"2024-03-19T13:32:08.399382Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training model with input shape: (128, 128, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 424ms/step - accuracy: 0.3243 - loss: 1.7343 - val_accuracy: 0.4205 - val_loss: 1.3548\nEpoch 2/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 422ms/step - accuracy: 0.5078 - loss: 1.2240 - val_accuracy: 0.5276 - val_loss: 1.1847\nEpoch 3/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 417ms/step - accuracy: 0.5852 - loss: 1.0545 - val_accuracy: 0.5640 - val_loss: 1.1111\nEpoch 4/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 427ms/step - accuracy: 0.6499 - loss: 0.9380 - val_accuracy: 0.5938 - val_loss: 1.0744\nEpoch 5/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 423ms/step - accuracy: 0.7046 - loss: 0.8063 - val_accuracy: 0.5949 - val_loss: 1.0595\nEpoch 6/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 421ms/step - accuracy: 0.7444 - loss: 0.6936 - val_accuracy: 0.6104 - val_loss: 1.0619\nEpoch 7/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 425ms/step - accuracy: 0.7915 - loss: 0.5855 - val_accuracy: 0.6181 - val_loss: 1.1390\nEpoch 8/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 427ms/step - accuracy: 0.8398 - loss: 0.4530 - val_accuracy: 0.5993 - val_loss: 1.3727\nEpoch 9/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 435ms/step - accuracy: 0.8610 - loss: 0.4085 - val_accuracy: 0.6060 - val_loss: 1.3410\nEpoch 10/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 430ms/step - accuracy: 0.9083 - loss: 0.2626 - val_accuracy: 0.5993 - val_loss: 1.4704\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.6176 - loss: 1.4427\nTest loss: 1.4704184532165527, Test accuracy: 0.5993377566337585\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (256, 256, 1)","metadata":{}},{"cell_type":"code","source":"# Define input size\ninput_shape = (256, 256, 1)\n\ntotal_images = 0\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        count = len(os.listdir(os.path.join(DATA_PATH, dir_)))\n        print(f\"{dir_} has {count} number of images\")\n        total_images += count\n\nprint(f\"\\ntotal images are {total_images}\")\n\nimg_arr = []\nimg_label = []\nlabel_to_text = {}\nlabel = 0\n\nfor dir_ in os.listdir(DATA_PATH):\n    if dir_ in TOP_EMOTIONS:\n        for f in os.listdir(os.path.join(DATA_PATH, dir_)):\n            img = cv2.imread(os.path.join(DATA_PATH, dir_, f), cv2.IMREAD_GRAYSCALE)\n            img = cv2.resize(img, input_shape[:2])  # Resize the image\n            img_arr.append(np.expand_dims(img, axis=-1))\n            img_label.append(label)\n        print(f\"loaded {dir_} images to numpy arrays...\")\n        label_to_text[label] = dir_\n        label += 1\n\nimg_arr = np.array(img_arr)\nimg_label = np.array(img_label)\nimg_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:32:08.402475Z","iopub.execute_input":"2024-03-19T13:32:08.402890Z","iopub.status.idle":"2024-03-19T13:32:16.398850Z","shell.execute_reply.started":"2024-03-19T13:32:08.402857Z","shell.execute_reply":"2024-03-19T13:32:16.397568Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fear has 552 number of images\nAngry has 633 number of images\nNeutral has 863 number of images\nSadness has 904 number of images\nHappy has 1577 number of images\n\ntotal images are 4529\nloaded Fear images to numpy arrays...\nloaded Angry images to numpy arrays...\nloaded Neutral images to numpy arrays...\nloaded Sadness images to numpy arrays...\nloaded Happy images to numpy arrays...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalize pixel values\nimg_arr = img_arr / 255.\n\nX_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=TRAIN_SIZE, random_state=42)\n\ndef create_cnn_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:32:16.400113Z","iopub.execute_input":"2024-03-19T13:32:16.402069Z","iopub.status.idle":"2024-03-19T13:32:21.824544Z","shell.execute_reply.started":"2024-03-19T13:32:16.402019Z","shell.execute_reply":"2024-03-19T13:32:21.823190Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:11:18.688940Z","iopub.execute_input":"2024-03-19T14:11:18.689358Z","iopub.status.idle":"2024-03-19T14:53:16.680465Z","shell.execute_reply.started":"2024-03-19T14:11:18.689308Z","shell.execute_reply":"2024-03-19T14:53:16.679019Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training model with input shape: (256, 256, 1)\nEpoch 1/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step - accuracy: 0.3404 - loss: 2.3096 - val_accuracy: 0.3985 - val_loss: 1.5024\nEpoch 2/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.4408 - loss: 1.3466 - val_accuracy: 0.5243 - val_loss: 1.2173\nEpoch 3/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 2s/step - accuracy: 0.5858 - loss: 1.0778 - val_accuracy: 0.5276 - val_loss: 1.2120\nEpoch 4/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 2s/step - accuracy: 0.6249 - loss: 0.9621 - val_accuracy: 0.5541 - val_loss: 1.1438\nEpoch 5/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 2s/step - accuracy: 0.7052 - loss: 0.8178 - val_accuracy: 0.5817 - val_loss: 1.1184\nEpoch 6/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.7468 - loss: 0.6925 - val_accuracy: 0.5640 - val_loss: 1.1781\nEpoch 7/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 2s/step - accuracy: 0.7910 - loss: 0.5880 - val_accuracy: 0.5894 - val_loss: 1.2497\nEpoch 8/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.8234 - loss: 0.4926 - val_accuracy: 0.5795 - val_loss: 1.3157\nEpoch 9/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.8789 - loss: 0.3595 - val_accuracy: 0.5773 - val_loss: 1.4911\nEpoch 10/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.9251 - loss: 0.2342 - val_accuracy: 0.5762 - val_loss: 1.6840\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 490ms/step - accuracy: 0.6003 - loss: 1.5676\nTest loss: 1.6840391159057617, Test accuracy: 0.5761589407920837\n","output_type":"stream"}]}]}