{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7777077,"sourceType":"datasetVersion","datasetId":4272440}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Define paths and target names\ndata_dir_train = \"/kaggle/input/fyp-dataset/RAFDB/RAFDB/train/\"\ndata_dir_test = \"/kaggle/input/fyp-dataset/RAFDB/RAFDB/test/\"\ntarget_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nNUM_CLASSES = len(target_names)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:59:36.642673Z","iopub.execute_input":"2024-03-19T15:59:36.643027Z","iopub.status.idle":"2024-03-19T15:59:53.788371Z","shell.execute_reply.started":"2024-03-19T15:59:36.642990Z","shell.execute_reply":"2024-03-19T15:59:53.787075Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-19 15:59:40.335203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-19 15:59:40.335479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-19 15:59:40.528580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (48, 48, 1)","metadata":{}},{"cell_type":"code","source":"# Function to load data\ndef load_data(data_dir, target_names, input_shape):\n    img_arr = []\n    img_label = []\n    label_to_text = {}\n    label = 0\n\n    for dir_ in os.listdir(data_dir):\n        if dir_ in target_names:\n            for f in os.listdir(data_dir + dir_ + \"/\"):\n                img = cv2.imread(data_dir + dir_ + \"/\" + f)\n                img = cv2.resize(img, input_shape[:2])  # Resize the image\n\n                # Convert image to grayscale if it's RGB\n                if img.shape[-1] == 3:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img_arr.append(img)\n                img_label.append(label)\n            print(f\"loaded {dir_} images to numpy arrays...\")\n            label_to_text[label] = dir_\n            label += 1\n\n    img_arr = np.array(img_arr)\n    img_label = np.array(img_label)\n    img_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array\n\n    return img_arr, img_label, label_to_text\n\n# Define input shape\ninput_shape = (48, 48, 1) # Update input shape based on loaded image size\n\n# Load training data\nimg_arr_train, img_label_train, label_to_text_train = load_data(data_dir_train, target_names, input_shape)\n\n# Load test data\nimg_arr_test, img_label_test, label_to_text_test = load_data(data_dir_test, target_names, input_shape)\n\n# Check shapes and labels\nprint(\"Training data shapes:\", img_arr_train.shape, img_label_train.shape, label_to_text_train)\nprint(\"Test data shapes:\", img_arr_test.shape, img_label_test.shape, label_to_text_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:05:05.410684Z","iopub.execute_input":"2024-03-19T19:05:05.412089Z","iopub.status.idle":"2024-03-19T19:05:40.519441Z","shell.execute_reply.started":"2024-03-19T19:05:05.412026Z","shell.execute_reply":"2024-03-19T19:05:40.518251Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"loaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nTraining data shapes: (12271, 48, 48) (12271, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\nTest data shapes: (3068, 48, 48) (3068, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_cnn_model(input_shape, NUM_CLASSES):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:05:40.521835Z","iopub.execute_input":"2024-03-19T19:05:40.522289Z","iopub.status.idle":"2024-03-19T19:05:40.530696Z","shell.execute_reply.started":"2024-03-19T19:05:40.522248Z","shell.execute_reply":"2024-03-19T19:05:40.529456Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\n\n# Preprocess the data (normalize pixel values)\n#img_arr_train = img_arr_train / 255.\n#img_arr_test = img_arr_test / 255.\n\n# Train the model\nmodel.fit(img_arr_train, img_label_train, epochs=10, validation_data=(img_arr_test, img_label_test))\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(img_arr_test, img_label_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:05:40.532215Z","iopub.execute_input":"2024-03-19T19:05:40.532571Z","iopub.status.idle":"2024-03-19T19:08:43.525968Z","shell.execute_reply.started":"2024-03-19T19:05:40.532543Z","shell.execute_reply":"2024-03-19T19:08:43.524905Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Training model with input shape: (48, 48, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - accuracy: 0.4192 - loss: 6.3354 - val_accuracy: 0.6033 - val_loss: 1.0983\nEpoch 2/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.6268 - loss: 1.0828 - val_accuracy: 0.6486 - val_loss: 0.9916\nEpoch 3/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.6690 - loss: 0.9242 - val_accuracy: 0.6516 - val_loss: 1.0132\nEpoch 4/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.7154 - loss: 0.8157 - val_accuracy: 0.6545 - val_loss: 1.0105\nEpoch 5/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.7452 - loss: 0.7257 - val_accuracy: 0.6467 - val_loss: 1.0607\nEpoch 6/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.7800 - loss: 0.6345 - val_accuracy: 0.6767 - val_loss: 1.0586\nEpoch 7/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.8057 - loss: 0.5469 - val_accuracy: 0.6467 - val_loss: 1.2522\nEpoch 8/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.8248 - loss: 0.4990 - val_accuracy: 0.6734 - val_loss: 1.2136\nEpoch 9/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.8521 - loss: 0.4257 - val_accuracy: 0.6626 - val_loss: 1.3266\nEpoch 10/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.8716 - loss: 0.3750 - val_accuracy: 0.6640 - val_loss: 1.5082\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6009 - loss: 2.0633\nTest loss: 1.5081573724746704, Test accuracy: 0.6639504432678223\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (64, 64, 1)","metadata":{}},{"cell_type":"code","source":"# Function to load data\ndef load_data(data_dir, target_names, input_shape):\n    img_arr = []\n    img_label = []\n    label_to_text = {}\n    label = 0\n\n    for dir_ in os.listdir(data_dir):\n        if dir_ in target_names:\n            for f in os.listdir(data_dir + dir_ + \"/\"):\n                img = cv2.imread(data_dir + dir_ + \"/\" + f)\n                img = cv2.resize(img, input_shape[:2])  # Resize the image\n\n                # Convert image to grayscale if it's RGB\n                if img.shape[-1] == 3:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img_arr.append(img)\n                img_label.append(label)\n            print(f\"loaded {dir_} images to numpy arrays...\")\n            label_to_text[label] = dir_\n            label += 1\n\n    img_arr = np.array(img_arr)\n    img_label = np.array(img_label)\n    img_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array\n\n    return img_arr, img_label, label_to_text\n\n# Define input shape\ninput_shape = (64, 64, 1) # Update input shape based on loaded image size\n\n# Load training data\nimg_arr_train, img_label_train, label_to_text_train = load_data(data_dir_train, target_names, input_shape)\n\n# Load test data\nimg_arr_test, img_label_test, label_to_text_test = load_data(data_dir_test, target_names, input_shape)\n\n# Check shapes and labels\nprint(\"Training data shapes:\", img_arr_train.shape, img_label_train.shape, label_to_text_train)\nprint(\"Test data shapes:\", img_arr_test.shape, img_label_test.shape, label_to_text_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:35:43.963936Z","iopub.execute_input":"2024-03-19T16:35:43.964422Z","iopub.status.idle":"2024-03-19T16:36:19.289671Z","shell.execute_reply.started":"2024-03-19T16:35:43.964387Z","shell.execute_reply":"2024-03-19T16:36:19.288232Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"loaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nTraining data shapes: (12271, 64, 64) (12271, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\nTest data shapes: (3068, 64, 64) (3068, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_cnn_model(input_shape, NUM_CLASSES):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:36:19.291806Z","iopub.execute_input":"2024-03-19T16:36:19.292324Z","iopub.status.idle":"2024-03-19T16:36:19.298450Z","shell.execute_reply.started":"2024-03-19T16:36:19.292290Z","shell.execute_reply":"2024-03-19T16:36:19.297680Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\n\n# Train the model\nmodel.fit(img_arr_train, img_label_train, epochs=10, validation_data=(img_arr_test, img_label_test))\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(img_arr_test, img_label_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:36:19.299791Z","iopub.execute_input":"2024-03-19T16:36:19.300231Z","iopub.status.idle":"2024-03-19T16:42:24.857713Z","shell.execute_reply.started":"2024-03-19T16:36:19.300177Z","shell.execute_reply":"2024-03-19T16:42:24.856877Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Training model with input shape: (64, 64, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 82ms/step - accuracy: 0.4441 - loss: 7.9696 - val_accuracy: 0.6141 - val_loss: 1.0936\nEpoch 2/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 82ms/step - accuracy: 0.6582 - loss: 0.9846 - val_accuracy: 0.6610 - val_loss: 0.9675\nEpoch 3/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 82ms/step - accuracy: 0.7182 - loss: 0.7941 - val_accuracy: 0.6675 - val_loss: 1.0041\nEpoch 4/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 80ms/step - accuracy: 0.7764 - loss: 0.6405 - val_accuracy: 0.6780 - val_loss: 0.9868\nEpoch 5/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.8276 - loss: 0.4997 - val_accuracy: 0.6662 - val_loss: 1.1100\nEpoch 6/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.8706 - loss: 0.3779 - val_accuracy: 0.6617 - val_loss: 1.2400\nEpoch 7/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 79ms/step - accuracy: 0.8988 - loss: 0.2941 - val_accuracy: 0.6519 - val_loss: 1.4204\nEpoch 8/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 0.9202 - loss: 0.2328 - val_accuracy: 0.6718 - val_loss: 1.4889\nEpoch 9/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 80ms/step - accuracy: 0.9267 - loss: 0.2043 - val_accuracy: 0.6581 - val_loss: 1.6848\nEpoch 10/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.9447 - loss: 0.1670 - val_accuracy: 0.6571 - val_loss: 1.8704\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.6224 - loss: 2.2719\nTest loss: 1.8704088926315308, Test accuracy: 0.6571056246757507\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (128, 128, 1)","metadata":{}},{"cell_type":"code","source":"# Function to load data\ndef load_data(data_dir, target_names, input_shape):\n    img_arr = []\n    img_label = []\n    label_to_text = {}\n    label = 0\n\n    for dir_ in os.listdir(data_dir):\n        if dir_ in target_names:\n            for f in os.listdir(data_dir + dir_ + \"/\"):\n                img = cv2.imread(data_dir + dir_ + \"/\" + f)\n                img = cv2.resize(img, input_shape[:2])  # Resize the image\n\n                # Convert image to grayscale if it's RGB\n                if img.shape[-1] == 3:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img_arr.append(img)\n                img_label.append(label)\n            print(f\"loaded {dir_} images to numpy arrays...\")\n            label_to_text[label] = dir_\n            label += 1\n\n    img_arr = np.array(img_arr)\n    img_label = np.array(img_label)\n    img_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array\n\n    return img_arr, img_label, label_to_text\n\n# Define input shape\ninput_shape = (128, 128, 1) # Update input shape based on loaded image size\n\n# Load training data\nimg_arr_train, img_label_train, label_to_text_train = load_data(data_dir_train, target_names, input_shape)\n\n# Load test data\nimg_arr_test, img_label_test, label_to_text_test = load_data(data_dir_test, target_names, input_shape)\n\n# Check shapes and labels\nprint(\"Training data shapes:\", img_arr_train.shape, img_label_train.shape, label_to_text_train)\nprint(\"Test data shapes:\", img_arr_test.shape, img_label_test.shape, label_to_text_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:42:24.859500Z","iopub.execute_input":"2024-03-19T16:42:24.860314Z","iopub.status.idle":"2024-03-19T16:42:43.989956Z","shell.execute_reply.started":"2024-03-19T16:42:24.860284Z","shell.execute_reply":"2024-03-19T16:42:43.988977Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"loaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nTraining data shapes: (12271, 128, 128) (12271, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\nTest data shapes: (3068, 128, 128) (3068, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_cnn_model(input_shape, NUM_CLASSES):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:42:43.991268Z","iopub.execute_input":"2024-03-19T16:42:43.992249Z","iopub.status.idle":"2024-03-19T16:42:44.000298Z","shell.execute_reply.started":"2024-03-19T16:42:43.992211Z","shell.execute_reply":"2024-03-19T16:42:43.999361Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\n\n# Train the model\nmodel.fit(img_arr_train, img_label_train, epochs=10, validation_data=(img_arr_test, img_label_test))\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(img_arr_test, img_label_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T16:42:44.001516Z","iopub.execute_input":"2024-03-19T16:42:44.002750Z","iopub.status.idle":"2024-03-19T17:06:28.370961Z","shell.execute_reply.started":"2024-03-19T16:42:44.002715Z","shell.execute_reply":"2024-03-19T17:06:28.369681Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Training model with input shape: (128, 128, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 368ms/step - accuracy: 0.4448 - loss: 22.1439 - val_accuracy: 0.6053 - val_loss: 1.1069\nEpoch 2/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 365ms/step - accuracy: 0.6664 - loss: 0.9537 - val_accuracy: 0.6232 - val_loss: 1.0631\nEpoch 3/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 367ms/step - accuracy: 0.7351 - loss: 0.7559 - val_accuracy: 0.6620 - val_loss: 1.0251\nEpoch 4/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 357ms/step - accuracy: 0.8073 - loss: 0.5516 - val_accuracy: 0.6369 - val_loss: 1.2296\nEpoch 5/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 367ms/step - accuracy: 0.8527 - loss: 0.4227 - val_accuracy: 0.6454 - val_loss: 1.3231\nEpoch 6/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 364ms/step - accuracy: 0.9046 - loss: 0.2821 - val_accuracy: 0.6258 - val_loss: 1.5850\nEpoch 7/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 357ms/step - accuracy: 0.9279 - loss: 0.2407 - val_accuracy: 0.6323 - val_loss: 1.8809\nEpoch 8/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 368ms/step - accuracy: 0.9560 - loss: 0.1402 - val_accuracy: 0.6444 - val_loss: 1.9108\nEpoch 9/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 368ms/step - accuracy: 0.9534 - loss: 0.1367 - val_accuracy: 0.6372 - val_loss: 2.3877\nEpoch 10/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 370ms/step - accuracy: 0.9688 - loss: 0.0913 - val_accuracy: 0.6359 - val_loss: 2.2604\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.5997 - loss: 2.8069\nTest loss: 2.260432481765747, Test accuracy: 0.6359191536903381\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define input size\ninput_shape = (256, 256, 1)","metadata":{}},{"cell_type":"code","source":"# Function to load data\ndef load_data(data_dir, target_names, input_shape):\n    img_arr = []\n    img_label = []\n    label_to_text = {}\n    label = 0\n\n    for dir_ in os.listdir(data_dir):\n        if dir_ in target_names:\n            for f in os.listdir(data_dir + dir_ + \"/\"):\n                img = cv2.imread(data_dir + dir_ + \"/\" + f)\n                img = cv2.resize(img, input_shape[:2])  # Resize the image\n\n                # Convert image to grayscale if it's RGB\n                if img.shape[-1] == 3:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img_arr.append(img)\n                img_label.append(label)\n            print(f\"loaded {dir_} images to numpy arrays...\")\n            label_to_text[label] = dir_\n            label += 1\n\n    img_arr = np.array(img_arr)\n    img_label = np.array(img_label)\n    img_label = OneHotEncoder().fit_transform(img_label.reshape(-1, 1)).toarray()  # Convert to dense array\n\n    return img_arr, img_label, label_to_text\n\n# Define input shape\ninput_shape = (256, 256, 1) # Update input shape based on loaded image size\n\n# Load training data\nimg_arr_train, img_label_train, label_to_text_train = load_data(data_dir_train, target_names, input_shape)\n\n# Load test data\nimg_arr_test, img_label_test, label_to_text_test = load_data(data_dir_test, target_names, input_shape)\n\n# Check shapes and labels\nprint(\"Training data shapes:\", img_arr_train.shape, img_label_train.shape, label_to_text_train)\nprint(\"Test data shapes:\", img_arr_test.shape, img_label_test.shape, label_to_text_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:06:28.372514Z","iopub.execute_input":"2024-03-19T17:06:28.373556Z","iopub.status.idle":"2024-03-19T17:07:09.666826Z","shell.execute_reply.started":"2024-03-19T17:06:28.373514Z","shell.execute_reply":"2024-03-19T17:07:09.665663Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"loaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nloaded surprise images to numpy arrays...\nloaded fear images to numpy arrays...\nloaded angry images to numpy arrays...\nloaded neutral images to numpy arrays...\nloaded sad images to numpy arrays...\nloaded disgust images to numpy arrays...\nloaded happy images to numpy arrays...\nTraining data shapes: (12271, 256, 256) (12271, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\nTest data shapes: (3068, 256, 256) (3068, 7) {0: 'surprise', 1: 'fear', 2: 'angry', 3: 'neutral', 4: 'sad', 5: 'disgust', 6: 'happy'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_cnn_model(input_shape, NUM_CLASSES):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:07:09.669081Z","iopub.execute_input":"2024-03-19T17:07:09.669748Z","iopub.status.idle":"2024-03-19T17:07:09.677120Z","shell.execute_reply.started":"2024-03-19T17:07:09.669709Z","shell.execute_reply":"2024-03-19T17:07:09.675939Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the model\nprint(f\"Training model with input shape: {input_shape}\")\nmodel = create_cnn_model(input_shape, NUM_CLASSES)\n\n# Train the model\nmodel.fit(img_arr_train, img_label_train, epochs=10, validation_data=(img_arr_test, img_label_test))\n\n# Evaluate the model on test data\nloss, accuracy = model.evaluate(img_arr_test, img_label_test)\nprint(f\"Test loss: {loss}, Test accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T17:07:09.678240Z","iopub.execute_input":"2024-03-19T17:07:09.678550Z","iopub.status.idle":"2024-03-19T19:02:00.386314Z","shell.execute_reply.started":"2024-03-19T17:07:09.678523Z","shell.execute_reply":"2024-03-19T19:02:00.383174Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training model with input shape: (256, 256, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 2s/step - accuracy: 0.3926 - loss: 93.6683 - val_accuracy: 0.3898 - val_loss: 1.6500\nEpoch 2/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 2s/step - accuracy: 0.4316 - loss: 1.5699 - val_accuracy: 0.4404 - val_loss: 1.5261\nEpoch 3/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2s/step - accuracy: 0.5471 - loss: 1.2614 - val_accuracy: 0.5023 - val_loss: 1.5581\nEpoch 4/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2s/step - accuracy: 0.6699 - loss: 0.9281 - val_accuracy: 0.4889 - val_loss: 1.6823\nEpoch 5/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 2s/step - accuracy: 0.7388 - loss: 0.7610 - val_accuracy: 0.5189 - val_loss: 2.1434\nEpoch 6/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2s/step - accuracy: 0.8033 - loss: 0.5700 - val_accuracy: 0.5267 - val_loss: 2.4872\nEpoch 7/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 2s/step - accuracy: 0.8600 - loss: 0.4142 - val_accuracy: 0.5271 - val_loss: 2.7262\nEpoch 8/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 2s/step - accuracy: 0.9014 - loss: 0.3030 - val_accuracy: 0.5453 - val_loss: 3.1744\nEpoch 9/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 2s/step - accuracy: 0.9131 - loss: 0.2874 - val_accuracy: 0.4961 - val_loss: 2.9315\nEpoch 10/10\n\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 2s/step - accuracy: 0.9118 - loss: 0.2856 - val_accuracy: 0.5231 - val_loss: 3.8042\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 429ms/step - accuracy: 0.4159 - loss: 5.2107\nTest loss: 3.8042447566986084, Test accuracy: 0.5231420993804932\n","output_type":"stream"}]}]}